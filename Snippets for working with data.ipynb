{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 6, 5  # plotsize \n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['image.cmap'] = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from pandas.tools.plotting import parallel_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.cluster import AgglomerativeClustering, MeanShift, KMeans, DBSCAN\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes / input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make new empty df\n",
    "df = pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(699, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000025</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         B  C  D  E  F  G  H  I  J  K\n",
       "1000025  5  1  1  1  2  1  3  1  1  2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('breast-cancer-wisconsin.data', names = ['B','C','D','E','F','G','H','I','J','K'],index_col = 0)\n",
    "# can also use read_sql, read_json, read_html, read_excel, read_table\n",
    "# can include parse_dates = [cols] and infer_datetime_format = True to read in dates\n",
    "# can specify which columns with usecols = []\n",
    "# can flag comments using comment = '#' for comments starting with #\n",
    "# can specify encoding, encoding = ''\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from StringIO import StringIO\n",
    "Data = 'a,b,c\\n1,2,3\\n4,5,6\\n7,8,9'\n",
    "df = pd.read_csv(StringIO(Data), dtype=object, usecols = [cols])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///:memory:', echo=False) # set echo to True to see output\n",
    "con = engine.connect()\n",
    "result = con.execute(query)\n",
    "\n",
    "df = pd.read_sql(sql, con)\n",
    "# sql is string SQL query or database table name\n",
    "# con is SQLAlchemy connectable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print column names, # non-nulls, and dtypes\n",
    "df.info()\n",
    "\n",
    "# return number of non-NaN entries\n",
    "non_nans = df.count()\n",
    "\n",
    "# remove rows or columns with NaN\n",
    "df.dropna(axis=0 or 1, how='any' or 'all', thresh=None or int, subset=None (rows or cols in non-axis dim), inplace=False)\n",
    "\n",
    "# return the number of counts for each of the values in a column\n",
    "df['B'].value_counts()\n",
    "\n",
    "# filling nans\n",
    "df.fillna('replacement')\n",
    "df.fillna(method = 'pad'/'bfill')\n",
    "df.fillna(df.mean())\n",
    "df.interpolate(method = 'time'/'values') #best for time series\n",
    "\n",
    "# return the indices for \"True\" values from the statement within the brackets, can apply any function, lambda function\n",
    "A = df.index[df['Bare_Nuclei'].apply(np.isnan)]\n",
    "\n",
    "# replace values in a column with specified values\n",
    "df['B'] = df['B'].replace('?',np.nan)\n",
    "\n",
    "# can also create a map (dict) to replace multiple values with new inputs\n",
    "map_dict = {2: 0, 4: 1}\n",
    "Y = df['K'].map(map_dict)\n",
    "\n",
    "# change the data type of a column\n",
    "df['B'] = df['B'] .astype('float')\n",
    "\n",
    "# drop rows\n",
    "X1 = df.drop('C',axis=1)\n",
    "# drop columns\n",
    "X2= df.drop(1,axis=0)\n",
    "\n",
    "# get column names as a list\n",
    "df.columns\n",
    "\n",
    "# choose n random samples\n",
    "df.sample(n)\n",
    "\n",
    "# remove outliers\n",
    "df[np.abs(df.Data-df.Data.mean())<=(3*df.Data.std())] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate\n",
    "pd.concat([df1,df2], axis=0, join='outer'/'inner', join_axes=[df1.index/df2.index], ignore_index=False, \n",
    "          keys=[hierarchical indexing])\n",
    "df1.append(df2, ignore_index=False/True) # adds rows of df2 to the bottom of df1\n",
    "\n",
    "# merge\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False)\n",
    "# how - inner, outer, left, right\n",
    "# on - cos to join on, must be in both dfs\n",
    "# left_on / right_on - cols from df_left / df_right to use as keys\n",
    "# left_index / right_index - if True, use as keys\n",
    "# sort - sort by join keys (set to False to speed up)\n",
    "pd.merge_ordered(left, right, fill_method='ffill', left_by='s')\n",
    "\n",
    "# join - on index as keys\n",
    "result = left.join(right,how='inner'/'outer', on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To geolocate a query to an address and coordinate\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "location = geolocator.geocode('27240 Altamont, Los Altos Hills')\n",
    "location.address\n",
    "location.latitude, location.longitude\n",
    "\n",
    "# to get an address from coordinates\n",
    "location = geolocator.reverse(\"52.509669, 13.376294\")\n",
    "\n",
    "# measuring distances\n",
    "from geopy.distance import vincenty\n",
    "location1 = (41.49008, -71.312796)\n",
    "location2 = (41.456, -72.3542)\n",
    "distance = vincenty(location1, location2).meters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to DatetimeIndex:\n",
    "pd.to_datetime(*args,**kwargs)\n",
    "    # can assemble from multiple columns, or from a series of strings, or tuples\n",
    "    # errors = 'coerce' to set invalid values as NaT\n",
    "    # format = '%d/%m/%Y'\n",
    "    # infer_datetime_format = True to speed up\n",
    "\n",
    "# make index with date ranges starting at start date and ending at end date or with P periods and frequency F\n",
    "index = pd.date_range('2000-1-1', periods=1000, freq='M')\n",
    "# for business days\n",
    "index = pd.bdate_range('2012-1-1', periods=250)\n",
    "\n",
    "pd.Period('2012-01-10', freq = 'D')\n",
    "# time span. specify the frequency / interval time\n",
    "\n",
    "pd.period_range()\n",
    "\n",
    "# indexing with datetime\n",
    "ts['10/31/2011':'12/31/2011'] # returns all rows between these dates\n",
    "# can also pass in the year or month as strings:\n",
    "ts['2011']\n",
    "ts['2011-6']\n",
    "dft[datetime(2013, 1, 1):datetime(2013,2,28)]\n",
    "\n",
    "# convert timestamps to period\n",
    "ts.to_period()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output a table of basic stats for each numeric column\n",
    "df.describe()\n",
    "\n",
    "# plot histogram of each column - if performed on grouped df, will automatically separate groups\n",
    "df[col].plot.hist(alpha = 0) # useful for comparing multiple groups on same axes\n",
    "df.hist() # plots hist for every column\n",
    "\n",
    "# plot bar chart of value counts\n",
    "df['col_name'].value_counts()[:10].plot(kind = 'bar')\n",
    "\n",
    "# plot cross-plots\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(df[], alpha = 1, diagonal = 'kde'/'hist')\n",
    "\n",
    "# parallel coordinates plot\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "parallel_coordinates(df[],'K')\n",
    "\n",
    "# hexbin plot for dense data\n",
    "plt.hexbin(a,b, gridsize = 50, cmap = 'viridis', C = df['col_name'], vmin = 1750, vmax = 1800,\n",
    "            reduce_C_function = np.min)\n",
    "plt.colorbar()\n",
    "\n",
    "# stacked area plot\n",
    "df.plot.area();\n",
    "\n",
    "# boxplot\n",
    " bp = df_box.boxplot(by='g') # grouped by col g\n",
    "    \n",
    "# get index of maximum/minimum value\n",
    "df.idxmax(axis=0)\n",
    "df.idxmin(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for plotting correlation heatmap\n",
    "def plot_corr_heatmap(corrs, labels, cmap='viridis'):\n",
    "    heatmap = plt.matshow(corrs, cmap=cmap, interpolation='nearest')\n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    plt.colorbar(label='Correlation', fraction=0.046, shrink=1.0)\n",
    "    plt.grid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs = df.corr(method='pearson')\n",
    "plot_corr_heatmap(corrs, df.columns.values, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group-by in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby('A') # splits on rows\n",
    "df.groupby(['A','B'])\n",
    "# can group by a user-defined function\n",
    "grouped = df.groupby(function,axis=1)\n",
    "# based on hierarchical indexing\n",
    "df.groupby(level = 0)\n",
    "\n",
    "# aggregate functions on grouped data\n",
    "grouped.first(), grouped.last(), grouped.sum()\n",
    "\n",
    "grouped.groups # returns a dictionary of groups\n",
    "\n",
    "# iterate through groups\n",
    "for name, group in df.groupby('A'):\n",
    "    print(name)\n",
    "    print(group)\n",
    "    \n",
    "# select a group\n",
    "grouped.get_group(label)\n",
    "\n",
    "grouped.size()\n",
    "grouped.describe()\n",
    "grouped.count()\n",
    "\n",
    "# apply a function to each group\n",
    "zscores = grouped.transform(lambda x: (x-x.mean())/x.std())\n",
    "imputed = grouped.transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# filter data based on group properties\n",
    "df.groupby('A').filter(lambda x: x.sum() > 2) # must return True or False when applied to group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test / train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "PCA(n_components=None, whiten=False, svd_solver='auto', tol=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set to zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "trainX = scale.fit_transform(trainX)\n",
    "testX = scale.transform(testX)\n",
    "# other methods include inverse_transform(X), partial_fit(X), get_params()\n",
    "\n",
    "# can also use RobustSaler, which is roubst to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general scikit-learn methods\n",
    "model = Model(*args)\n",
    "model.fit(X)\n",
    "model.fit_predict(X)\n",
    "model.get_params()\n",
    "model.predict(X)\n",
    "model.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "KMeans(n_clusters = 8, tol=0.0001)\n",
    "\n",
    "# mean shift\n",
    "from sklearn.cluster import MeanShift\n",
    "MeanShift(bandwidth = None, seeds = None, cluster_all = True)\n",
    "\n",
    "# agglomerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean'/'l1'/'l2'/'cosine', \n",
    "                        linkage = 'ward'/'average')\n",
    "\n",
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "DBSCAN(eps=0.5, min_samples = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier(n_neighbors = 5, weights='uniform'/'distance')\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel = 'rbf'/'linear'/'poly',probability=True, class_weight = 'balanced')\n",
    "\n",
    "# Logistic Regression\n",
    "from sklenar.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(penalty = 'l1'/'l2', multi_class='ovr'/'multinomial')\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifer\n",
    "RF = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, oob_score=True)\n",
    "\n",
    "# Boosted Trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "BT = GradientBoostingClassifier(loss = 'deviance'/'exponential', learning_rate = 0.1,\n",
    "                                n_estimators = 100,max_depth = 3, min_samples_split = 2)\n",
    "# deviance - logistic regression, exponential - adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import\n",
    "LinearRegression()\n",
    "LassoCV()\n",
    "RidgeCV()\n",
    "ElasticNetCV(l1_ratio = 0.5, cv = n_folds, normalize = True/False)\n",
    "\n",
    "from sklearn.ensemble import\n",
    "RandomForestRegressor(n_estimators=10, max_depth=None,min_samples_split=2, oob_score=True)\n",
    "GradientBoostingRegressor(loss='ls'/'lad'/'huber',learning_rate = 0.1, n_estimators=100,min_samples_split=2,max_depth=3)\n",
    "AdaBoostRegressor(base_estimator=DecisionTreeRegressor, n_estimators=50, learning_rate=1.0,\n",
    "                 loss='linear'/'square'/'exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklenar.model_selection import GridSearchCV\n",
    "# param_grid is a dict with param names as keys and lists of param settings as values\n",
    "gsearch = GridSearchCV(estimator, param_grid, cv = n_folds)\n",
    "gsearch.cv_results_\n",
    "gsearch.best_estimator_\n",
    "gsearch.best_score_\n",
    "gsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification\n",
    "sklearn.metrics.accuracy_score(y_true,y_pred)\n",
    "sklearn.metrics.auc(x,y) # computes area under any curve\n",
    "sklearn.metrics.confusion_matrix(y_true,y_pred)\n",
    "sklearn.metrics.precision_score(y_true,y_pred)\n",
    "sklearn.metrics.recall_score(y_true,y_pred)\n",
    "\n",
    "# regression\n",
    "sklearn.metrics.explained_variance_score(y_true, y_pred)\n",
    "sklearn.metrics.mean_absolute_error(y_true, y_pred)\n",
    "sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "sklearn.metrics.r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_stats(model, X_test, y_test):\n",
    "    predicted = model.predict(X_test)\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "    classes = [int(v) for v in list(set(y_test))]\n",
    "    \n",
    "    # Get Accuracy and ROC AUC results for each class individually\n",
    "    start = 0\n",
    "    if probabilities.shape[1] == 2:\n",
    "        start = 1\n",
    "    for i in range(start, probabilities.shape[1]):\n",
    "        probs = probabilities[:,i]\n",
    "        current_class = classes[i]\n",
    "        y_test_i = [1 if current_class == int(v) else 0 for v in y_test]\n",
    "        predicted_i = [1 if current_class == int(v) else 0 for v in predicted]\n",
    "        print('Class {}'.format(current_class))\n",
    "        print('Accuracy: {:0.2f}'.format(sklearn.metrics.accuracy_score(y_test_i, predicted_i)))\n",
    "        print('ROC AUC Score: {:0.2f}'.format(sklearn.metrics.roc_auc_score(y_test_i, probs)))\n",
    "        print()\n",
    "        \n",
    "    print('Confusion Matrix')\n",
    "\n",
    "    # Print out confusion matrix legend  if only 2 classes\n",
    "    if len(classes) == 2:\n",
    "        print('True Negative (Guess 0, Actual 0)  | False Positive (Guess 1, Actual 0)')\n",
    "        print('-----------------------------------------------------------------------')\n",
    "        print('False Negative (Guess 0, Actual 1) |  True Positive (Guess 1, Actual 1)')\n",
    "        print()\n",
    "\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, predicted))\n",
    "    print()\n",
    "    print('Classification Report')\n",
    "    print(sklearn.metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_auc(model, X_test, y_test):\n",
    "    predicted = model.predict(X_test)\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "    classes = list(set(y_test))\n",
    "    \n",
    "    # Get ROC curve for each class individually\n",
    "    start = 0\n",
    "    if probabilities.shape[1] == 2:\n",
    "        start = 1\n",
    "    for i in range(start, probabilities.shape[1]):\n",
    "        probs = probabilities[:,i]\n",
    "        current_class = classes[i]\n",
    "        y_test_i = [1 if current_class == int(v) else 0 for v in y_test]\n",
    "    \n",
    "        false_positive_rate, true_positive_rate, thresholds = sklearn.metrics.roc_curve(y_test_i, probs)\n",
    "        roc_auc = sklearn.metrics.auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "        plt.plot(false_positive_rate, true_positive_rate,\n",
    "            label='{} - AUC = {:0.2f}'.format(current_class, roc_auc))\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator,\n",
    "        X, y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        # scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
